# Introduction to SEO and Content Indexing #

Just a few days ago, [Google](http://google.com "Google Search Engine") started indexing [this blog](http://www.ponyfoo.com/ "Pony Foo"), and its *just starting to show up* in their search results. I wanted to mention the _careful steps_ I took to make a good impression on them.

I must open mentioning I'm no expert nor authority on SEO, **I just read a lot**.

The absolute first step was *making the markup semantic*. That is, using **HTML 5** tags, such as `<article>`, `<section>`, `<header>`, `<footer>` and so on. For a _full list of HTML 5 tags_, [visit MDN](https://developer.mozilla.org/en-US/docs/HTML/HTML5/HTML5_element_list "HTML 5 Tag List").

This helps crawlers assign **weight** (importance) to each piece of HTML in your page. It makes your pages _future-proof_, so that if crawlers start giving more importance to semantic markup, you are ready for it. And _it makes your CSS more semantic, too_, which makes it more intuitive to modify.

The second step I took was adding [Google Analytics](http://www.google.com/analytics/ "Google Web Analytics") to my solution. I later went on and added a couple of other services: [Clicky](http://clicky.com "Clicky Web Analytics") and [New Relic](http://newrelic.com/ "New Relic Application Monitoring), to improve my analytics and have at least some sort of _uptime monitoring_. All these services are really easy to include _effortlessly_ in your application, and they provide **a lot of value**.

Analytics can tell you _what pages_ users land on, what pages are the _most linked to_, where your users _come from_, as well as _how your users behave_ and what they are looking for. In summary, it's really important to know what's going on with your site in the grand scheme of things, and figure out how to proceed, and analytics tools are a _great way_ to accomplish just that.

# AJAX Crawling #

The next step was the most complex of them. Since this site is entirely AJAX, _meaning users navigating only load the site once_, the page is for all intents and purposes _static_, meaning that no matter what page you go to, you get _the same HTML_, and then JavaScript takes care of displaying the appropriate view. This is clearly a **dealbreaking issue** for web scrapers, because they cannot index your site if _all your pages look the same_.

> Without a proper AJAX crawling strategy, any other efforts to improve SEO are **utterly useless**. You need _a good crawling strategy_ that allows search engines to get the content a regular surfer would find in each page.

The desired behavior would, then, be some sort of reflection of this _mockup_:

![crawler.png][1]
  
### Implementation ###

In my research, I figured out a [headless browser](http://zombie.labnotes.org/ "Zombie.js headless browser") was the way to go. A **headless browser** is basically a way to make a request to a page _and execute any JavaScript_ in it, without resorting to a _full-fledged_ web browser. This would allow me to **transparently** serve web crawlers with static versions of the dynamic pages on my blog, without having to resort to _obscure techniques_ or manually editing the static versions of the site.

Once I had this down, it was just a matter of adding a little helper to _every single **GET** route_ to handle crawler requests differently. If a request comes in and it matches one of the known web crawler user agents, a _second request_ is triggered on behalf of the crawler, against the same resource, and through the headless browser.

After waiting for all the JavaScript to get executed, and after a little cleanup (since the page is static, it makes sense to me to _remove all script tags_ before serving it), we are ready to serve this view to the crawler agent.

One last step if you care about performance is dumping this into a file cache that is relatively **short-lived** (meaning you'll _invalidate that cached page_ if a determined amount of time elapses), in order to save yourself a web request in subsequent calls made by a crawler agent against that resource.

If you are curious about how to implement this, [here is my take for this blog](https://github.com/bevacqua/NBrut/blob/v0.2/src/logic/zombie.js "Crawler AJAX Support Implementation"), it is implemented in Node.JS.

Note that _this might not be the latest version_. It's the one contained in the **v0.2** tag. Although I don't expect to change it much.

> Once that's settled, and working, you can do awesome stuff such as updating your `<meta>` tags through JavaScript, and the web crawlers will pick up on it!

- robots
- sitemap
- meta
- OG
- schema.org

  [1]: http://i.imgur.com/7E8mifH.png "Desired behavior"